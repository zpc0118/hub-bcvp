from collections import defaultdict

def get_pairs_count(tokens):
    stats = defaultdict(int)
    for i in range(len(tokens) - 1):
        stats[(tokens[i], tokens[i + 1])] += 1
    return stats

def merge_pairs(tokens, pair, new_token_id):
    new_tokens = []
    i = 0
    while i < len(tokens):
        if i <  len(tokens) - 1 and tokens[i] == pair[0] and tokens[i + 1] == pair[1]:
            new_tokens.append(new_token_id)
            i += 2
        else:
            new_tokens.append(tokens[i])
            i += 1
    return new_tokens

def encode(text, vocab_table):
    tokens = list(text.encode("utf-8"))  # 0~255åŸå§‹æ•°å­—åºåˆ—
    # æ ¹æ®è¯è¡¨å¯¹åŸå§‹æ•°å­—åºåˆ—åšæ›¿æ¢ï¼ˆå‹ç¼©ï¼‰
    while len(tokens) >= 2:
        stats = get_pairs_count(tokens)
        # åœ¨ stats çš„æ‰€æœ‰å…ƒç´ ä¸­ï¼Œæ‰¾å‡ºé‚£ä¸ªåœ¨è¯è¡¨é‡Œå¯¹åº”token_idçš„å€¼æœ€å°çš„äºŒå…ƒç»„
        pair = None
        min_idx = float("inf")
        for p, count in stats.items():
            idx = vocab_table.get(p, float("inf"))
            if idx < min_idx:
                pair = p
                min_idx = idx
        # å¦‚æœstatsé‡Œä¸å­˜åœ¨è¯è¡¨vocab_tableä¸­çš„äºŒå…ƒç»„ï¼Œç›´æ¥åœæ­¢æ›¿æ¢
        if pair is None:
            break
        # idx = vocab_table[pair]
        tokens = merge_pairs(tokens, pair, min_idx)
    return tokens

def decode(encoded_tokens, vocab_table):
    # vocab = {idx: bytes([idx]) for idx in range(256)}
    # å…ˆåœ¨0~255çš„åŸºç¡€ä¸Šæ·»åŠ è¯è¡¨ä¸­çš„æ–°token_id
    vocab = defaultdict(bytes)
    for idx in range(256):
        vocab[idx] = bytes([idx])
    for (p0, p1), idx in vocab_table.items():
        vocab[idx] = vocab[p0] + vocab[p1]
    tokens = b"".join(vocab[idx] for idx in encoded_tokens)   # b"".join(...)ï¼šå°†æ‰€æœ‰byteså¯¹è±¡æ‹¼æ¥æˆä¸€ä¸ªå®Œæ•´çš„bytes
    text = tokens.decode("utf-8", errors="replace")    # errors="replace", æ›¿æ¢å­—ç¬¦ã€‚å°†æ— æ³•è§£ç çš„å­—èŠ‚æ›¿æ¢ä¸º ``
    return text


if __name__ == "__main__":

    text = (
            "ã€Šé‡‘å¶åƒè°œæ¡ˆã€‹ï¼ŒåŸåã€ŠThe Case of the Golden Idolã€‹ï¼Œæ˜¯ç”±Color Gray Gameså¼€å‘çš„ä¸€æ¬¾æ¨ç†æ¸¸æˆã€‚è¯¥å·¥ä½œå®¤å·²ç»æ¨å‡ºå…¶ç»­ä½œã€Šé‡‘å¶åƒå´›èµ·ã€‹ï¼Œæœ€è¿‘æ¨å‡ºdlcï¼Œä¸¤æ¬¾æ¸¸æˆéƒ½æ˜¯5æŠ˜æ–°å²ä½ã€‚"
            "ã€Šé‡‘å¶åƒè°œæ¡ˆã€‹æœ¬ä½“è®²è¿°äº†12ä¸ªå‘ç”Ÿåœ¨18ä¸–çºªï¼Œå‰åè·¨è¶Š50å¹´çš„æ­»äº¡æ¡ˆä»¶ï¼Œè¿™äº›æ¡ˆä»¶éƒ½ä¸ä»å¼‚å›½è€Œæ¥çš„é‡‘å¶åƒæœ‰ç€å¯†ä¸å¯åˆ†çš„å…³ç³»ï¼Œæ¡ˆä»¶é€æ­¥æ·±åŒ–ï¼Œæœ€åä¸Šå‡è‡³æ”¿æ²»å±‚é¢ã€‚"
            "ã€Šé‡‘å¶åƒè°œæ¡ˆã€‹çš„ä¸¤ä¸ªdlcã€Šå…°å¡çš„èœ˜è››ã€‹ã€Šé‡Œè«åˆ©äºšå¸è¡€é¬¼ã€‹è¡¥å…¨äº†æœ¬ä½“çš„å‰§æƒ…ï¼Œç›¸å½“äºæœ¬ä½“çš„å‰ä¼ ï¼Œä½¿å¾—æœ¬ä½œçš„å‰§æƒ…å®Œæˆé—­ç¯ã€‚"
            
            "ã€Šé‡‘å¶åƒè°œæ¡ˆã€‹çš„æ¨ç†è¿‡ç¨‹æ˜¯è·å–ä¿¡æ¯çš„è¿‡ç¨‹ï¼Œç©å®¶åœ¨å›ºå®šçš„åœºæ™¯é€šè¿‡ä¸ç‰©ä½“ï¼Œäººç‰©äº’åŠ¨è·å–ä¿¡æ¯ï¼Œå¤§éƒ¨åˆ†ä¿¡æ¯ä¼šä»¥å…³é”®è¯çš„å½¢å¼å‚¨å­˜ï¼Œç”¨ä»¥å¡«å†™å·è½´ã€‚ã€Šé‡‘å¶åƒè°œæ¡ˆã€‹çš„ç›®æ ‡å°±æ˜¯å¡«å†™å·è½´ï¼Œå·è½´å¤§ä½“å¯ä»¥æ¦‚æ‹¬ä¸ºå‡ ä¸ªéƒ¨åˆ†ï¼š"
            "æ¡ˆä»¶ç»¼è¿°ï¼Œäººç‰©èº«ä»½ï¼Œæ¡ˆæƒ…ç›¸å…³å†…å®¹ã€‚å¡«å†™çš„å†…å®¹ä¸åˆ†å…ˆåé¡ºåºï¼Œè¿™æ„å‘³ç€ç©å®¶å¯ä»¥æŒ‰ç…§ä¸ªäººæ€è€ƒæ–¹å¼è¿›è¡Œæ¸¸æˆã€‚å½“ä¸€ä¸ªéƒ¨åˆ†çš„æ‰€æœ‰ç©ºç™½éƒ½è¢«å¡«å†™åï¼Œæ¸¸æˆä¼šæç¤ºå½“å‰éƒ¨åˆ†æ˜¯å¦å­˜åœ¨é”™è¯¯ï¼Œåˆ†åˆ«ä¸ºï¼šå·è½´å¡«å†™å­˜åœ¨é”™è¯¯å’Œå·è½´å¡«å†™å­˜åœ¨è‡³å¤šä¸¤ä¸ªé”™è¯¯ï¼Œ"
            "ç©å®¶å¯ä»¥æ ¹æ®æç¤ºå®Œå–„æ¨ç†å†…å®¹ï¼ŒåŒæ—¶ï¼Œæ¸¸æˆè¿˜æœ‰æç¤ºç³»ç»Ÿï¼Œå¯ä»¥è¿›ä¸€æ­¥ç®€åŒ–å†…å®¹ã€‚"
            
            "ã€Šé‡‘å¶åƒè°œæ¡ˆã€‹çš„æ¸¸æˆæ–¹å¼ç®€åŒ–äº†æ­£å¸¸æ¨ç†æ¸¸æˆä¸­ç©å®¶è·å–ä¿¡æ¯çš„è¿‡ç¨‹ï¼Œä»…éœ€å®Œå…¨æµè§ˆå®Œåœºæ™¯ä¿¡æ¯å°±å¯ä»¥å¾—åˆ°å…¨éƒ¨å…³é”®è¯ã€‚ã€Šé‡‘å¶åƒè°œæ¡ˆã€‹çš„æ¨ç†è¿‡ç¨‹ä¸ã€Šå¥¥ä¼¯æ‹‰ä¸çš„å›å½’ã€‹æœ‰ç›¸ä¼¼ä¹‹å¤„ï¼Œ"
            "éƒ½æ˜¯åœ¨å›ºå®šçš„åœºæ™¯ä¹‹ä¸­æœå¯»ä¿¡æ¯ï¼Œå¹¶ä¸”å¡«å†™çš„å†…å®¹æœ¬èº«å¸¦æœ‰ä¸€å®šçš„æç¤ºã€‚ã€Šé‡‘å¶åƒè°œæ¡ˆã€‹çš„ä¿¡æ¯è·å–é™¤å¼€å¯ä»¥â€œæ”¶åˆ°èƒŒåŒ…â€ä¸­çš„å…³é”®è¯å¤–ï¼Œåœºæ™¯ã€äººç‰©ã€ç‰©ä»¶æœ¬èº«ä¹Ÿå¸¦æœ‰çº¿ç´¢ï¼Œè¿™ä½¿å¾—ã€Šé‡‘å¶åƒè°œæ¡ˆã€‹é€»è¾‘æ›´åŠ å®Œå¤‡ã€‚"
            "æ­¤å¤–ï¼Œåœ¨æ¸¸æˆè¿‡ç¨‹ä¸­ï¼Œå½“ä½ å¡«å†™80%çš„å†…å®¹åï¼Œå¦‚æœæ²¡æœ‰è¿›ä¸€æ­¥çš„æ€è·¯ï¼Œå¯ä»¥é‡‡ç”¨æšä¸¾æ³•è¯•å‡ºç­”æ¡ˆï¼Œä¹‹åå¯ä»¥å†å»ç†æ¸…æ€è·¯ã€‚åœ¨æ¸¸ç©è¿‡ç¨‹ä¸­ï¼Œç©å®¶é¢ä¸´çš„æœ€å¤§é—®é¢˜å¯èƒ½æ˜¯äººåçš„è®°å¿†ï¼Œå¤§é‡äººåéå¸¸å®¹æ˜“è®°ä¸æ¸…ï¼Œ"
            "å¯èƒ½ä¼šéœ€è¦åå¤ç¡®è®¤ã€‚ä¸è¿‡ã€Šé‡‘å¶åƒè°œæ¡ˆã€‹çš„æ¸¸ç©æ—¶é•¿å¹¶ä¸é•¿ï¼Œæœ¬ä½“åŠ å…¨dlcçš„æ¸¸ç©æ—¶é•¿å¤§æ¦‚åœ¨6åˆ°10å°æ—¶ï¼Œåœ¨ç©å®¶å¯¹äººåä¸å¤§åœºæ™¯è°ƒæŸ¥æ„Ÿåˆ°åŒçƒ¦ä¹‹å‰ï¼Œæ¸¸æˆå·²ç»å°†å†…å®¹è®²è¿°å®Œæ¯•ã€‚")

    # 1. utf8ç¼–ç 
    tokens = text.encode("utf-8")  # raw bytes
    tokens = list(tokens)
    print("åŸæ–‡é•¿åº¦ï¼š", len(text))
    print(tokens)
    print("utf8ç¼–ç åé•¿åº¦ï¼š", len(tokens))
    print("=" * 60)

    # 2. ç»Ÿè®¡æ¯ä¸ª2å…ƒç»„å‡ºç°æ¬¡æ•°
    stats = get_pairs_count(tokens)
    print("ç»Ÿè®¡æ¯ä¸ª2å…ƒç»„å‡ºç°æ¬¡æ•°ï¼š", sorted(stats.items(), key=lambda x: x[1], reverse=True))
    print("=" * 60)

    # 3. é€‰å–å‡ºç°æ¬¡æ•°æœ€å¤šçš„2å…ƒç»„ï¼Œåˆå¹¶ï¼Œæ„å»ºè¯è¡¨ï¼ˆæ›¿æ¢æ˜ å°„è¡¨ï¼‰
    vocab_size = 300  # the desired final vocabulary size  è¶…å‚æ•°ï¼šé¢„æœŸçš„æœ€ç»ˆè¯è¡¨å¤§å°ï¼Œæ ¹æ®å®é™…æƒ…å†µè‡ªå·±è®¾ç½®ï¼Œå¤§çš„è¯è¡¨ä¼šéœ€è¦å¤§çš„embeddingå±‚
    num_merges = vocab_size - 256  # è¦åˆå¹¶çš„æ¬¡æ•°
    compressed_tokens = tokens.copy()   # å¤åˆ¶ä¸€ä»½

    merge_table = defaultdict(int)  # bpeåˆå¹¶ï¼ˆå‹ç¼©ï¼‰è¿‡ç¨‹ä¸­å½¢æˆçš„è¯è¡¨ï¼ˆæ›¿æ¢æ˜ å°„è¡¨ï¼‰
    for i in range(num_merges):
        stats = get_pairs_count(compressed_tokens)
        # æ‰¾å‡ºå½“å‰ç»Ÿè®¡æ¬¡æ•°æœ€é«˜çš„äºŒå…ƒç»„
        max_freq_pair = max(stats, key=stats.get)
        # æ›¿æ¢
        new_idx = 256 + i
        print(f"æŠŠäºŒå…ƒç»„ {max_freq_pair} æ›¿æ¢ä¸º {new_idx}")
        compressed_tokens = merge_pairs(compressed_tokens, max_freq_pair, new_idx)
        merge_table[max_freq_pair] = new_idx   # è®°å½•è¯è¡¨

    print("---")
    print("åŸå§‹tokensé•¿åº¦:", len(tokens))
    print("bpeå‹ç¼©åtokensé•¿åº¦:", len(compressed_tokens))
    print(f"å‹ç¼©æ¯”ç‡: {len(tokens) / len(compressed_tokens):.2f}X")
    print("==" * 60)

    # 4. å¾—åˆ°è¯è¡¨ merge_tableï¼Œæ ¹æ®è¯è¡¨å°è¯•å°†ä¸€å¥æ–‡æœ¬encodeç¼–ç ä¸º0~255æ•°å­—çš„è¡¨ç¤ºåºåˆ—
    sentence = "ã€Šé‡‘å¶åƒè°œæ¡ˆã€‹çš„æ¨ç†è¿‡ç¨‹ä¸ã€Šå¥¥ä¼¯æ‹‰ä¸çš„å›å½’ã€‹æœ‰ç›¸ä¼¼ä¹‹å¤„ï¼Œéƒ½æ˜¯åœ¨å›ºå®šçš„åœºæ™¯ä¹‹ä¸­æœå¯»ä¿¡æ¯ï¼Œå¹¶ä¸”å¡«å†™çš„å†…å®¹æœ¬èº«å¸¦æœ‰ä¸€å®šçš„æç¤ºã€‚ã€Šé‡‘å¶åƒè°œæ¡ˆã€‹çš„ä¿¡æ¯è·å–é™¤å¼€å¯ä»¥â€œæ”¶åˆ°èƒŒåŒ…â€ä¸­çš„å…³é”®è¯å¤–ï¼Œåœºæ™¯ã€äººç‰©ã€ç‰©ä»¶æœ¬èº«ä¹Ÿå¸¦æœ‰çº¿ç´¢ï¼Œè¿™ä½¿å¾—ã€Šé‡‘å¶åƒè°œæ¡ˆã€‹é€»è¾‘æ›´åŠ å®Œå¤‡ã€‚"
    # sentence = "Hello World!"
    # sentence = "ğ ®·ğ¬º“è¦…æ¢»ğªœ€ğ«œ´ğ«  _(:Ğ·ã€âˆ )_ğŸ¨ğ ‘Šå˜¦çƒéº¤åšé"
    encoded_tokens = encode(sentence, merge_table)
    print("åŸæ–‡:", sentence)
    print("encoded_tokens:", encoded_tokens)
    print("=" * 60)

    # 5. æ ¹æ®è¯è¡¨ merge_tableï¼Œå†å°†å‹ç¼©åçš„tokens 0~255æ•°å­—åºåˆ—è¿˜åŸä¸ºåŸæ–‡
    decoded_sentence = decode(encoded_tokens, merge_table)
    print("encoded_tokens:", encoded_tokens)
    print("è¿˜åŸ:", decoded_sentence)
    print("=" * 60)

    sentence2 = decode(encode(sentence, merge_table), merge_table)
    print(sentence2 == sentence)
